להלן תוכנית מפורטת לשדרוג המערכת כך שה־orchestrator יקבל "מוח" LLM ויוכל לנתח בקשות בשפה טבעית ולהפנות אותן לכלי המתאים:

1. מטרה – שדרוג ה־Orchestrator עם LLM
אנחנו רוצים לעבור ממערכת ניתוב פשוטה (מבוססת על בדיקה סטטית של מילים) למערכת חכמה יותר שבה:
ה־Orchestrator ישלח את הודעות המשתמש ל־LLM.
ה־LLM יקבל את ההודעה בשפה טבעית ויניב פלט מובנה (למשל, JSON) שמתאר:
איזה Agent צריך לטפל בבקשה (למשל, info, action, research).
איזה פעולה לבצע יחד עם הפרמטרים הדרושים (כמו, "מציאה", "עדכון", "יצירה" ועוד).
על סמך הפלט, המערכת תזהה באיזה Agent להשתמש ותעביר לו את הפרמטרים המתאימים.
בנוסף, נדון בשאלה האם כל Agent צריך לקבל את יכולות LLM משלו או שריכוז ה־LLM ב־Orchestrator מספיק עבור רוב המקרים.
---
2. שלבים לפיתוח ולשדרוג
שלב 2.1 – בניית מודול LLM (LLM Client)
יצירת/עדכון מודול LLM (לדוגמא: tools/llm_api.py):
ננצל את הממשק הקיים לקריאה ל־LLM (אנחנו כבר עובדים עם Anthropic/OpenAI).
נגדיר פונקציה שתקבל prompt ותוחזר תשובה. חשוב להגדיר לפלט תבנית מובנה (למשל JSON) שתכיל את:
ה-agent (למשל: "info", "action", "research")
הפעולה (למשל: "get_products", "update_inventory", "create_product")
הפרמטרים (לדוגמא: { "product_name": "XYZ", "price": "100.00" })
דוגמא לקוד:

import requests
import os
import json

def query_llm(prompt: str, provider: str = "anthropic") -> dict:
    """
    שולח prompt ל-LLM ומחזיר את התגובה כמילון (מפורמט JSON).
    ודאו שאתם מריצים את הפונקציה בתוך סביבת venv.
    """
    # כאן נבצע קריאה ל-LLM API
    # דוגמה עם Anthropic (התאם לפי ההגדרות שלך)
    api_url = os.getenv("LLM_API_URL")
    api_key = os.getenv("LLM_API_KEY")
    
    headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}
    data = {"prompt": prompt, "max_tokens": 150}
    
    response = requests.post(api_url, headers=headers, json=data)
    response.raise_for_status()
    return response.json()


שלב 2.2 – עדכון ה-Orchestrator לשימוש ב-LLM
עדכון מתודת handle_user_message ב־Orchestrator:
במקום לבדוק מילים מתוך הטקסט, העבירו את הטקסט ל־LLM יחד עם prompt קבוע שמנחה את המודל להחזיר פלט מובנה (למשל JSON עם השדות agent, method, ו-params).
פרש את התגובה והפנה את הבקשה ל־Agent המתאים.
דוגמא לקוד:

from src.agents.information_agent import InformationAgent
from src.agents.action_agent import ActionAgent
from src.agents.research_agent import ResearchAgent
from tools.llm_api import query_llm
import json

class Orchestrator:
    def __init__(self):
        self.info_agent = InformationAgent()
        self.action_agent = ActionAgent()
        self.research_agent = ResearchAgent()
    
    def prepare_prompt(self, user_message: str) -> str:
        """
        בונה prompt למודל, כך שהפלט יהיה בפורמט JSON מובנה.
        ההוראות למודל יכולים לכלול:
        - בחר את ה-agent המתאים (info, action, research)
        - בחר את המתודה להתקשרות עם ה-agent, כמו get_products, create_product, update_inventory וכו'.
        - החזר JSON עם השדות: agent, method, params
        לדוגמה:
        {
          "agent": "action",
          "method": "create_product",
          "params": {
              "name": "...",
              "price": "..."
          }
        }
        """
        instructions = (
            "בבקשה נתח את הבקשה הבאה ובחר את הסוכן המתאים לפי הפעולה המבוקשת. "
            "החזר את הפלט כ-JSON עם השדות: agent, method, params. "
            "לדוגמא, עבור בקשה ליצור מוצר, החזר: "
            '{"agent": "action", "method": "create_product", "params": {"name": "שם מוצר", "price": "מחיר"}}. '
            "בבקשה ענה רק בפורמט JSON ללא טקסט נוסף.\n\n'
        )
        return instructions + f"הודעת המשתמש: {user_message}"
    
    def handle_user_message(self, user_message: str) -> str:
        prompt = self.prepare_prompt(user_message)
        try:
            # קראו ל-LLM לקבלת הפלט המבנה
            llm_output = query_llm(prompt)
            # נניח שהפלט כבר בפורמט JSON או מחרוזת שניתן להמיר למילון:
            if isinstance(llm_output, str):
                command = json.loads(llm_output)
            else:
                command = llm_output
            
            agent = command.get("agent")
            method = command.get("method")
            params = command.get("params", {})
            
            if agent == "info":
                response = self.info_agent.handle_message(method, params)
            elif agent == "action":
                response = self.action_agent.handle_message(method, params)
            elif agent == "research":
                response = self.research_agent.handle_message(method, params)
            else:
                response = "Orchestrator: לא ברור על מה תרצה לדבר."
        except Exception as e:
            response = f"שגיאה בעיבוד הפקודה: {str(e)}"
        return response


־Agents לקבלת קריאות ממובנות
עדכנו את מתודת handle_message בכל Agent לקבל את הפקודה המבנית:
לדוגמא, במחלקת ActionAgent:
בדקו את הערך של method והפעלו את הפונקציה המתאימה עם params.
בצורה זו תוכלו להרחיב בהדרגה ולממש פונקציונליות אמיתית לכל פעולה.
דוגמא (פשטני):

class ActionAgent:
    def handle_message(self, method: str, params: dict) -> str:
        if method == "create_product":
            # קריאה למתודה המתאימה, למשל:
            return self.create_product(params)
        # הוספת מתודות נוספות לפי הצורך
        return "ActionAgent: לא ידוע באיזה פעולה לבצע."
    
    def create_product(self, params: dict) -> str:
        # נניח params מכילים שם, מחיר וכו'
        # ביצוע קריאת POST ל-WooCommerce ליצירת מוצר
        try:
            response = self.wcapi.post("products", params)
            if response.status_code == 201:
                return f"מוצר '{params.get('name')}' נוצר בהצלחה."
            else:
                return f"שגיאה ביצירת מוצר: {response.status_code}"
        except Exception as e:
            return f"שגיאה ביצירת מוצר: {str(e)}"




            3. האם כל Agent צריך לקבל "מוח LLM" משלו?
גישה מרכזית:
עדיף שרק ה־Orchestrator, שמרכז את ניתוח הבקשה והניתוב, יקבל גישה ל-LLM. כך:
יינתן מקום למודל להיות מופעל פעם אחת ולהחזיר תשובות מובנות.
ה־Agents יישארו בעלי לוגיקה עסקית פשוטה – הם מבצעים פעולות מוגדרות (CRUD, עדכון מלאי, וכו׳).
גישה מבוזרת:
במקרים בהם Agent מסוים עוסק בהחלטות מורכבות (למשל, ביצוע ניתוח שוק או המלצות מוצר), ניתן לשקול להוסיף גם ל-Agent עצמו אפשרות לבצע קריאות ל-LLM לעיבוד תתי-בקשות.
המלצתי: ראשית, להתמקד ביכולת ניתוח במרכז (Orchestrator) ולאחר מכן, לפי הצורך, ניתן להרחיב יכולות בבקרים ספציפיים.
---
4. שלבי יישום והכנסה לפעולה
הגדרת תבניות prompt:
לערוך תבנית אחידה שתשמש את ה־LLM ליצירת JSON מובנה.
לערוך בדיקות בעזרת הפקת דוגמאות (מבחינת Anthropic agents blog ודוגמאות שהפנינו) שמראות איך הגיוני לקלוט פקודות.
כתיבת בדיקות אינטגרציה:
לעדכן את הבדיקות (unit/integration) כך שיבדקו את תהליך הניתוב: המשתמש שולח הודעה בשפה טבעית, ה־LLM מחזיר JSON, וה־Agent המתאים מופעל.
לדוגמה, לבדיקה שפונקציית handle_user_message מחזירה נכון מחרוזת ממוצר.
ריצה בסביבת בדיקה (Sandbox):
להריץ את כל הקריאות בתוך הסביבה venv כפי שמוגדר, ולוודא שלא נגרמות טעויות בהרצת API חיצוניים.
4. בדיקות אמינות וקונסיסטנטיות:
לוודא שהפלט של הפונקציית LLM קבוע ואינו כולל טקסט נוסף (על ידי הגדרת ההוראות במדויק).
לעדכן לוגים (כפי שמופיעים כבר בקבצים כמו logs/app.log ו-logs/action_agent.log) לצורך ניטור ותפעול.
הדרכה והמשך שיפור:
להתחיל ביישום בסיסי בכדי לקבל "גרסת MVP" (מוצר ראשוני) וכך לקבל פידבק מהמשתמש.
התאמה ושיפור התבניות והפונקציות על סמך התנסות.
---
5. סיכום
חיבור LLM ל-Orchestrator:
באמצעות שילוב מודול LLM במרכז, נוכל לשפר את יכולת ניתוח ההודעות בשפה טבעית ולהפנות את הפקודות למודולים המתאימים.
חלוקת תפקידים:
אין צורך לתת לכל Agent "מוח" נפרד – לרוב, ניתוח הבקשה המרכזי והניתוב יעשו אצל ה־Orchestrator, וה־Agents יתמקצעו בפעולות העסקיות השונות.
תוכנית שלב אחר שלב:
יצרנו תכנית מפורטת הכוללת:
יצירת מודול LLM ל-query ולפלט מובנה.
עדכון ה-Orchestrator לשימוש בצד LLM.
3. עדכון כל Agent לקבל פקודות מובנות בשילוב עם לוגיקה עסקית.
כתיבת בדיקות אינטגרציה ובדיקה במערך Sandbox.
סיכום הלמידה ושיפור מתמיד של המערכת.
אסטרטגיה זו תעניק למערכת את היכולת להבין בקשות מורכבות בשפה טבעית ולהפנות אותן בצורה מדויקת לכלי המתאים – מה שיתרום מאוד לשדרוג הפרוייקט, ובמיוחד לבינה המלאכותית הנדרשת לניהול חנות מקוונת בצורה חכמה ויעילה.
בהצלחה בשדרוג!